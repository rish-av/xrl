from .transformer_models import TransformerEncoder, TransformerDecoder, TransformerEncoderPerStates, TransformerDecoderPerState, TransformerDecoderAtari, TransformerEncoderAtari
import torch.nn as nn
import torch

class BeXRLSequence(nn.Module):
    def __init__(self, input_dim, model_dim, output_dim, num_heads, num_encoder_layers, num_decoder_layers, num_embeddings):
        super(BeXRLSequence, self).__init__()
        self.encoder = TransformerEncoder(input_dim, model_dim, num_heads, num_encoder_layers, num_embeddings)
        self.decoder = TransformerDecoder(model_dim, output_dim, num_heads, num_decoder_layers)
    
    def forward(self, src, tgt):
        memory, vq_loss, encidx = self.encoder(src)  # Only the final quantized state and VQ loss from encoder
        output = self.decoder(tgt, memory)  # Decode future states from the quantized encoder output
        return output, vq_loss, encidx 

    def generate_sequence(encoder, decoder, input_seq, max_length):
        memory, vq_loss, _ = encoder(input_seq)
        generated_seq = []
        next_input = memory.unsqueeze(1)  # Start with the end-of-sequence embedding from the encoder

        for _ in range(max_length):
            output = decoder(next_input, memory.unsqueeze(0))  # Shape: (batch_size, 1, output_dim)
            next_token = output[:, -1, :]  # Shape: (batch_size, output_dim)
            generated_seq.append(next_token)
            next_input = next_token.unsqueeze(1)  # Shape: (batch_size, 1, output_dim)
        generated_seq = torch.cat(generated_seq, dim=1)  # Shape: (batch_size, max_length, output_dim)
        return generated_seq, vq_loss 

class BeXRLState(nn.Module):
    def __init__(self, input_dim, model_dim, output_dim, num_heads, num_encoder_layers, num_decoder_layers, num_embeddings):
        super(BeXRLState, self).__init__()
        self.encoder = TransformerEncoderPerStates(input_dim, model_dim, num_heads, num_encoder_layers, num_embeddings)
        self.decoder = TransformerDecoderPerState(model_dim, output_dim, num_heads, num_decoder_layers)
    
    def forward(self, src, tgt):
        memory, vq_loss, encidx = self.encoder(src)  # Only the final quantized state and VQ loss from encoder
        output = self.decoder(tgt, memory)  # Decode future states from the quantized encoder output
        return output, vq_loss, encidx  



class BeXRLSequenceAtari(nn.Module):
    def __init__(self, input_channels, action_dim, model_dim, output_channels, num_heads, num_encoder_layers, num_decoder_layers, num_embeddings, frame_size=(84, 84)):
        super(BeXRLSequenceAtari, self).__init__()
        self.encoder = TransformerEncoderAtari(input_channels, action_dim, model_dim, num_heads, num_encoder_layers, num_embeddings, frame_size)
        self.decoder = TransformerDecoderAtari(model_dim, output_channels, num_heads, num_decoder_layers, frame_size)

    def forward(self, src_states, src_actions, tgt_states):
        memory, vq_loss, encidx = self.encoder(src_states, src_actions)
        output = self.decoder(tgt_states, memory)
        return output, vq_loss, encidx

    @staticmethod
    def generate_sequence(encoder, decoder, initial_state, initial_action, max_length, frame_size=(84, 84)):
        # Get memory from the encoder based on initial state and action sequence
        memory, vq_loss, _ = encoder(initial_state, initial_action)
        generated_seq = []
        next_input_state = initial_state[:, -1:]  # Start with the last frame of the initial state sequence
        next_input_action = initial_action[:, -1:]  # Start with the last action of the initial action sequence

        for _ in range(max_length):
            # Decode next frame using the previous output as the new input state-action pair
            output = decoder(next_input_state, next_input_action, memory)

            # Get the last frame generated by the decoder
            next_frame = output[:, -1, :, :, :]  # Shape: (batch_size, channels, height, width)
            generated_seq.append(next_frame)

            # Use the generated frame and a corresponding action as the next input
            next_input_state = next_frame.unsqueeze(1)  # Shape: (batch_size, 1, channels, height, width)
            next_input_action = torch.zeros_like(next_input_action)  # Dummy action for simplicity; can be modified as needed

        # Concatenate all generated frames along the sequence dimension
        generated_seq = torch.cat(generated_seq, dim=1)  # Shape: (batch_size, max_length, channels, height, width)
        return generated_seq, vq_loss